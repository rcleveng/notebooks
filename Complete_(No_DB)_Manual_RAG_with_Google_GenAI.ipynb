{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+uadhj6p5siEVk/ieSSZ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rcleveng/notebooks/blob/main/Complete_(No_DB)_Manual_RAG_with_Google_GenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple RAG langchain example using Google's Generate AI API.\n",
        "This example let's you ask questions against a book.\n",
        "\n",
        "You'll need an API key from Generative AI Studio.\n"
      ],
      "metadata": {
        "id": "2Y7r3isO2tpz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AVlTEDo1lD2"
      },
      "outputs": [],
      "source": [
        "!pip install langchain google.generativeai langchain-google-genai ratelimit more_itertools\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Restart the kernel once dependencies are updated"
      ],
      "metadata": {
        "id": "LFDEmAZJqav1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)\n"
      ],
      "metadata": {
        "id": "eju7mBv_qbRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a220b60-57b4-4cfa-f9c0-2e2da0accdf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start Milvus vector database that is used for holding the vectors."
      ],
      "metadata": {
        "id": "I3VeMwXgqbqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set the book URL to use for Q&A\n",
        "\n",
        "# Change this to use a different book.\n",
        "BOOK_URL=\"https://www.gutenberg.org/cache/epub/3300/pg3300.txt\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "bLb55wTgtRe5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Fetch and split URL into chunks\n",
        "\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "\n",
        "# Langchain does a reasonable job splitting easily, so we'll use that\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.schema import Document\n",
        "\n",
        "\n",
        "# Get text from URL\n",
        "def get_data(url):\n",
        " r = requests.get(url)\n",
        " return r.text\n",
        "\n",
        "BOOK_TEXT = get_data(BOOK_URL)\n",
        "doc =  Document(page_content=BOOK_TEXT, metadata={\"source\": \"local\"})\n",
        "text_splitter = RecursiveCharacterTextSplitter (chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents([doc])\n",
        "\n",
        "# Count the number of chunks\n",
        "print ('Finished splitting document into ', len(texts), 'chunks.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_TI1UC50YaZ",
        "outputId": "69b06d74-2ae9-46e7-9156-b8a81c2959f3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished splitting document into  3102 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from google.colab import userdata\n",
        "from more_itertools import batched\n",
        "\n",
        "\n",
        "import requests\n",
        "\n",
        "clean_texts = list(map(lambda x: x.page_content, texts))\n",
        "google_api_key = userdata.get('PALM_KEY')\n",
        "genai.configure(api_key=google_api_key)\n",
        "\n",
        "\n",
        "Count = 1\n",
        "@sleep_and_retry\n",
        "@limits(calls=1500, period=60)\n",
        "def batch_embed_fn(text):\n",
        "  global Count\n",
        "  Count += 1\n",
        "  if Count % 100 == 0:\n",
        "    print(\"Batch\", Count)\n",
        "  return genai.embed_content(model=\"models/embedding-001\",\n",
        "                             content=text,\n",
        "                             task_type=\"retrieval_document\")[\"embedding\"]\n",
        "\n",
        "\n",
        "embeddings = []\n",
        "for batch in batched(clean_texts, 50):\n",
        "  emb_batch = batch_embed_fn(batch)\n",
        "  embeddings.extend(emb_batch)\n",
        "\n",
        "\n",
        "d = { 'id' : range(0, len(clean_texts)),\n",
        "      'text' : clean_texts,\n",
        "      'embeddings' : embeddings }\n",
        "df = pd.DataFrame(d)\n",
        "\n",
        "print('Dataframe created for ', len(clean_texts), ' items.')\n"
      ],
      "metadata": {
        "id": "J05XHZM3k1K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df\n"
      ],
      "metadata": {
        "id": "R83Dif0qyFtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ask Questions on The Wealth of Nations\n",
        "import numpy as np\n",
        "\n",
        "# Ask question\n",
        "question = \"What is the title of the book?\" #@param {type:\"string\"}\n",
        "\n",
        "# emedded query\n",
        "emb_question = genai.embed_content(model=\"models/embedding-001\",\n",
        "                                   content=question,\n",
        "                                   task_type=\"retrieval_query\")[\"embedding\"]\n",
        "\n",
        "print(\"emb_question: \", type(emb_question), emb_question)\n",
        "\n",
        "\n",
        "# Compute cosine similarity\n",
        "cosine_similarities = np.dot(embeddings, emb_question).flatten()\n",
        "\n",
        "# Find the index of the most similar sentence\n",
        "most_similar_index = np.argmax(cosine_similarities)\n",
        "topk = np.argsort(cosine_similarities)[-10:]\n",
        "for k in topk:\n",
        "  print('\\n\\n***********************\\nArticle id: ', k, ': ', clean_texts[k])\n",
        "\n",
        "print(f\"\\n\\n***********************\\nMost similar sentence: {clean_texts[most_similar_index]}\")\n",
        "\n",
        "\n",
        "\n",
        "context = \"\\n\".join(map(lambda k: clean_texts[k], topk))\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try\n",
        "to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Helpful Answer:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\\n***********************\\nUsing full prompt: \", prompt)\n",
        "answer = \"\"\n",
        "\n",
        "model_name = 'models/text-bison-001' # @param [\"gemini-pro\", \"models/text-bison-001\"] {allow-input: false}\n",
        "\n",
        "# emedded query\n",
        "if model_name == \"gemini-pro\":\n",
        "  model = genai.GenerativeModel('gemini-pro')\n",
        "  answer = model.generate_content(question).text\n",
        "else:\n",
        "  c = genai.generate_text(model=model_name, prompt=question, max_output_tokens=2000)\n",
        "  answer = c.result\n",
        "\n",
        "print(\"answer: \", answer)\n"
      ],
      "metadata": {
        "id": "3sSzi99SAwMm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}